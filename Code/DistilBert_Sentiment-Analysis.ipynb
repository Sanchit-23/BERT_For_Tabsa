{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-29T12:06:26.627601Z","iopub.execute_input":"2022-05-29T12:06:26.627976Z","iopub.status.idle":"2022-05-29T12:06:26.638150Z","shell.execute_reply.started":"2022-05-29T12:06:26.627943Z","shell.execute_reply":"2022-05-29T12:06:26.636951Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:06:26.640152Z","iopub.execute_input":"2022-05-29T12:06:26.640735Z","iopub.status.idle":"2022-05-29T12:06:36.074540Z","shell.execute_reply.started":"2022-05-29T12:06:26.640666Z","shell.execute_reply":"2022-05-29T12:06:36.073488Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!pip install torch","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:06:36.078325Z","iopub.execute_input":"2022-05-29T12:06:36.078661Z","iopub.status.idle":"2022-05-29T12:06:45.910946Z","shell.execute_reply.started":"2022-05-29T12:06:36.078626Z","shell.execute_reply":"2022-05-29T12:06:45.909850Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport random\nimport json\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:06:45.913973Z","iopub.execute_input":"2022-05-29T12:06:45.914698Z","iopub.status.idle":"2022-05-29T12:06:45.919048Z","shell.execute_reply.started":"2022-05-29T12:06:45.914660Z","shell.execute_reply":"2022-05-29T12:06:45.918269Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def convert_input(data):\n  for entry in data:\n    entry[\"text\"]=entry[\"text\"].replace(\"LOCATION2\",\"location - 2\").replace(\"LOCATION1\",\"location - 1\")\n    for opinion in entry[\"opinions\"]:\n      opinion[\"target_entity\"]=opinion[\"target_entity\"].replace(\"LOCATION2\",\"location - 2\").replace(\"LOCATION1\",\"location - 1\")\n      opinion[\"aspect\"]=opinion[\"aspect\"].replace(\"transit-location\",\"transit location\")\n  return data","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:06:45.921431Z","iopub.execute_input":"2022-05-29T12:06:45.922055Z","iopub.status.idle":"2022-05-29T12:06:45.929885Z","shell.execute_reply.started":"2022-05-29T12:06:45.922020Z","shell.execute_reply":"2022-05-29T12:06:45.929116Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"sentihood_locations=[\"location - 1\",\"location - 2\"]\nsentihood_aspects=[\"general\",\"price\",\"safety\",\"transit location\"]\nsentihood_sentiments=[\"none\",\"positive\",\"negative\"]\nsentihood_label2id = {\"none\": 0, \"positive\": 1, \"negative\": 2}\nid2label = {0: \"None\", 1: \"Positive\", 2: \"Negative\"}\nlabel2id = {\"None\": 0, \"Positive\": 1, \"Negative\": 2}","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:08:38.139765Z","iopub.execute_input":"2022-05-29T12:08:38.140412Z","iopub.status.idle":"2022-05-29T12:08:38.145730Z","shell.execute_reply.started":"2022-05-29T12:08:38.140378Z","shell.execute_reply":"2022-05-29T12:08:38.144901Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def generate_sentiments_NLI_M(data):\n    output = []\n    for entry in data:\n        id = entry[\"id\"]\n        original_sentence = entry[\"text\"]\n        for location in sentihood_locations:\n            if location in original_sentence:\n                for aspect in sentihood_aspects:\n                    auxiliary_sentence = f\"{location} - {aspect}\"\n                    label = \"none\"\n                    for opinion in entry[\"opinions\"]:\n                        if opinion[\"target_entity\"] == location and opinion[\"aspect\"] == aspect:\n                            if opinion[\"sentiment\"] == \"Positive\":\n                                label = \"positive\"\n                            elif opinion[\"sentiment\"] == \"Negative\":\n                                label = \"negative\"\n                    output.append([id, original_sentence, auxiliary_sentence, sentihood_label2id[label], label])\n    loc1_rows = sorted([row for row in output if \"location - 1\" in row[2]], key=lambda el: el[0])\n    loc2_rows = sorted([row for row in output if \"location - 2\" in row[2]], key=lambda el: el[0])\n    output = [[\"id\", \"original_sentence\", \"auxiliary_sentence\", \"label_id\", \"label\"]]\n    output += loc1_rows + loc2_rows\n    df = pd.DataFrame(output)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:08:41.816979Z","iopub.execute_input":"2022-05-29T12:08:41.817368Z","iopub.status.idle":"2022-05-29T12:08:41.827262Z","shell.execute_reply.started":"2022-05-29T12:08:41.817336Z","shell.execute_reply":"2022-05-29T12:08:41.826422Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"with open(\"../input/jsonfile/jsonvalidator.json\",\"r\") as f:\n  data = json.loads(f.read())\n  data = convert_input(data)\n  df=generate_sentiments_NLI_M(data)\n  df.to_csv(\"auxilary_NLIM.csv\",index=False,header=False)\n\nauxdata=pd.read_csv(r\"./auxilary_NLIM.csv\",index_col=0)\nauxdata.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:08:54.364832Z","iopub.execute_input":"2022-05-29T12:08:54.365476Z","iopub.status.idle":"2022-05-29T12:08:54.539100Z","shell.execute_reply.started":"2022-05-29T12:08:54.365440Z","shell.execute_reply":"2022-05-29T12:08:54.538240Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"with open(\"../input/jsonfile/jsonvalidator_test.json\",\"r\") as f:\n  data = json.loads(f.read())\n  data = convert_input(data)\n  df=generate_sentiments_NLI_M(data)\n  df.to_csv(\"auxilary_NLIM_test.csv\",index=False,header=False)\nauxdata_test=pd.read_csv(r\"./auxilary_NLIM_test.csv\",index_col=0)\nauxdata_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:08:54.541044Z","iopub.execute_input":"2022-05-29T12:08:54.541450Z","iopub.status.idle":"2022-05-29T12:08:54.646052Z","shell.execute_reply.started":"2022-05-29T12:08:54.541409Z","shell.execute_reply":"2022-05-29T12:08:54.645176Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"with open(\"../input/jsonfile/jsonvalidator_val.json\",\"r\") as f:\n  data = json.loads(f.read())\n  data = convert_input(data)\n  df=generate_sentiments_NLI_M(data)\n  df.to_csv(\"auxilary_NLIM_val.csv\",index=False,header=False)\nauxdata_val=pd.read_csv(r\"./auxilary_NLIM_val.csv\",index_col=0)\nauxdata_val.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:08:54.647686Z","iopub.execute_input":"2022-05-29T12:08:54.648075Z","iopub.status.idle":"2022-05-29T12:08:54.706034Z","shell.execute_reply.started":"2022-05-29T12:08:54.648038Z","shell.execute_reply":"2022-05-29T12:08:54.705163Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train_original_sentences = list(auxdata.iloc[:,0])\ntrain_auxiliary_sentences = list(auxdata.iloc[:,1])\ntrain_labels = list(auxdata.iloc[:,2])\ntest_original_sentences = list(auxdata_test.iloc[:,0])\ntest_auxiliary_sentences = list(auxdata_test.iloc[:,1])\ntest_labels = list(auxdata_test.iloc[:,2])\nval_original_sentences = list(auxdata_val.iloc[:,0])\nval_auxiliary_sentences = list(auxdata_val.iloc[:,1])\nval_labels = list(auxdata_val.iloc[:,2])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:08:54.708331Z","iopub.execute_input":"2022-05-29T12:08:54.708692Z","iopub.status.idle":"2022-05-29T12:08:54.726410Z","shell.execute_reply.started":"2022-05-29T12:08:54.708656Z","shell.execute_reply":"2022-05-29T12:08:54.725487Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertTokenizerFast\n\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\ntrain_encodings = tokenizer(train_original_sentences, train_auxiliary_sentences, truncation=True, padding=True)\nval_encodings = tokenizer(val_original_sentences,val_auxiliary_sentences, truncation=True, padding=True)\ntest_encodings = tokenizer(test_original_sentences,test_auxiliary_sentences, truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:08:54.727523Z","iopub.execute_input":"2022-05-29T12:08:54.727955Z","iopub.status.idle":"2022-05-29T12:09:16.648943Z","shell.execute_reply.started":"2022-05-29T12:08:54.727919Z","shell.execute_reply":"2022-05-29T12:09:16.648092Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import torch\n\nclass ABSA_Dataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = ABSA_Dataset(train_encodings, train_labels)\nval_dataset=ABSA_Dataset(val_encodings, val_labels)\ntest_dataset=ABSA_Dataset(test_encodings, test_labels)\n\n# print(list(torch.utils.data.DataLoader(train_dataset, batch_size=2)))\n# print(ABSA_Dataset()[0])\n\ndataset = train_dataset\n  \n# get the first sample and unpack\n# first_data = dataset[1]\n# features = first_data\n# print(features)\nprint(len(dataset[0]))","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:09:16.650519Z","iopub.execute_input":"2022-05-29T12:09:16.651185Z","iopub.status.idle":"2022-05-29T12:09:16.659964Z","shell.execute_reply.started":"2022-05-29T12:09:16.651145Z","shell.execute_reply":"2022-05-29T12:09:16.659090Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.special import softmax\ndef get_predictions(data):\n    predicted_labels = []\n    scores = []\n   \n    data = pd.read_csv(data, header=0).values.tolist()\n    for row in data:\n        \n        predicted_labels.append(int(row[0]))\n        scores.append([float(el) for el in row[1:]])\n    return predicted_labels, scores","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:09:16.661347Z","iopub.execute_input":"2022-05-29T12:09:16.661699Z","iopub.status.idle":"2022-05-29T12:09:16.672233Z","shell.execute_reply.started":"2022-05-29T12:09:16.661662Z","shell.execute_reply":"2022-05-29T12:09:16.671385Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\n\ndef compute_sentihood_aspect_strict_accuracy(test_labels, predicted_labels):\n    correct_count = 0\n    num_examples = len(test_labels) // 4\n    for i in range(num_examples-4):\n        if test_labels[i * 4] == predicted_labels[i * 4]\\\n                and test_labels[i * 4 + 1] == predicted_labels[i * 4 + 1]\\\n                and test_labels[i * 4 + 2] == predicted_labels[i * 4 + 2]\\\n                and test_labels[i * 4 + 3] == predicted_labels[i * 4 + 3]:\n            correct_count += 1\n    return correct_count / num_examples\n\n\ndef compute_sentihood_aspect_macro_F1(test_labels, predicted_labels):\n    total_precision = 0\n    total_recall = 0\n    num_examples = len(test_labels) // 4\n    count_examples_with_sentiments = 0\n    for i in range(num_examples-4):\n        test_aspects = set()\n        predicted_aspects = set()\n        for j in range(4):\n            if test_labels[i * 4 + j] != 0:\n                test_aspects.add(j)\n            if predicted_labels[i * 4 + j] != 0:\n                predicted_aspects.add(j)\n        if len(test_aspects) == 0:\n            continue\n        intersection = test_aspects.intersection(predicted_aspects)\n        if len(intersection) > 0:\n            precision = len(intersection) / len(predicted_aspects)\n            recall = len(intersection) / len(test_aspects)\n        else:\n            precision = 0\n            recall = 0\n        total_precision += precision\n        total_recall += recall\n        count_examples_with_sentiments += 1\n    ma_P = total_precision / count_examples_with_sentiments\n    ma_R = total_recall / count_examples_with_sentiments\n    return (2 * ma_P * ma_R) / (ma_P + ma_R)\n\n\ndef compute_sentihood_aspect_macro_AUC(test_labels, scores):\n    aspects_test_labels = [[] for _ in range(4)]\n    aspects_none_scores = [[] for _ in range(4)]\n    for i in range(len(test_labels)):\n        if test_labels[i] != 0:\n            new_label = 0\n        else:\n            new_label = 1   # For metrics.roc_auc_score you need to use the score of the maximum label, so \"None\" : 1\n        aspects_test_labels[i % 4].append(new_label)\n        aspects_none_scores[i % 4].append(scores[i][0])\n    aspect_AUC = []\n    for i in range(4):\n        aspect_AUC.append(metrics.roc_auc_score(aspects_test_labels[i], aspects_none_scores[i]))\n    aspect_macro_AUC = np.mean(aspect_AUC)\n    return aspect_macro_AUC\n\n\ndef compute_sentihood_sentiment_classification_metrics(test_labels, scores):\n    \"\"\"Compute macro AUC and accuracy for sentiment classification ignoring \"None\" scores\"\"\"\n    # Macro AUC\n    sentiment_test_labels = [[] for _ in range(4)]  # One list for each aspect\n    sentiment_negative_scores = [[] for _ in range(4)]\n    sentiment_predicted_label = []\n    sentiment_test_label = []   # One global list\n    for i in range(len(test_labels)):\n        if test_labels[i] != 0:\n            new_test_label = test_labels[i] - 1  # \"Positive\": 0, \"Negative\": 1\n            sentiment_test_label.append(new_test_label)\n            new_negative_score = scores[i][2] / (scores[i][1] + scores[i][2])   # Prob. of \"Negative\" ignoring \"None\"\n            if new_negative_score > 0.5:\n                sentiment_predicted_label.append(1)\n            else:\n                sentiment_predicted_label.append(0)\n            sentiment_test_labels[i % 4].append(new_test_label)\n            sentiment_negative_scores[i % 4].append(new_negative_score)\n    sentiment_AUC = []\n    for i in range(4):\n        sentiment_AUC.append(metrics.roc_auc_score(sentiment_test_labels[i], sentiment_negative_scores[i]))\n    sentiment_macro_AUC = np.mean(sentiment_AUC)\n\n    # Accuracy\n    sentiment_accuracy = metrics.accuracy_score(sentiment_test_label, sentiment_predicted_label)\n\n    return sentiment_macro_AUC, sentiment_accuracy\n\n\ndef compute_metrics(predictions):\n        scores = [softmax(prediction) for prediction in predictions[0]]\n        predicted_labels = [np.argmax(x) for x in scores]\n        print(len(predicted_labels))\n        data = np.insert(scores, 0, predicted_labels, axis=1)\n        predicted_labels, scores = get_predictions(data)\n        test_labels1 = test_labels\n        metrics = {}\n        metrics[\"strict_acc\"] = compute_sentihood_aspect_strict_accuracy(test_labels1, predicted_labels)\n        metrics[\"F1\"] = compute_sentihood_aspect_macro_F1(test_labels1, predicted_labels)\n        metrics[\"aspect_AUC\"] = compute_sentihood_aspect_macro_AUC(test_labels1, scores)\n        sentiment_macro_AUC, sentiment_accuracy = compute_sentihood_sentiment_classification_metrics(test_labels1, scores)\n        metrics[\"sentiment_acc\"] = sentiment_accuracy\n        metrics[\"sentiment_AUC\"] = sentiment_macro_AUC\n        return metrics","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:09:16.673746Z","iopub.execute_input":"2022-05-29T12:09:16.674365Z","iopub.status.idle":"2022-05-29T12:09:17.288688Z","shell.execute_reply.started":"2022-05-29T12:09:16.674328Z","shell.execute_reply":"2022-05-29T12:09:17.287787Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertForSequenceClassification , Trainer, TrainingArguments, DistilBertConfig\n\nfrom transformers import logging\nlogging.set_verbosity_debug()\n\n\nepochs = 4\nbatch_size = 24\nnum_steps = len(train_dataset) * epochs // batch_size\nwarmup_steps = num_steps // 10  # 10% of the training steps\nsave_steps = num_steps // epochs    # Save a checkpoint at the end of each epoch\n\n\ntraining_args = TrainingArguments(\n    output_dir = r\"./\",          \n    num_train_epochs = epochs,              \n    per_device_train_batch_size = batch_size,  \n    per_device_eval_batch_size = batch_size,   \n    warmup_steps = warmup_steps,   \n    weight_decay = 0.01,               \n    logging_dir = r\"./\",            \n    logging_steps = 10,\n    evaluation_strategy = 'epoch',\n    learning_rate = 2e-5,\n    save_steps = save_steps\n)\n\nconfig = DistilBertConfig.from_pretrained(\n    'distilbert-base-uncased',\n    architectures = ['DistilBertForSequenceClassification'],\n    hidden_size = 768,\n    num_hidden_layers = 6,\n    num_attention_heads = 12,\n    hidden_dropout_prob = 0.1,\n    num_labels = 3\n)    \n\nload_finetuned_model = False\nif not load_finetuned_model:\n    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', config=config)\n\n    trainer = Trainer(\n        model=model,                         \n        args=training_args,                  \n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        #compute_metrics=compute_metrics                     \n    )\n    trainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:09:17.290297Z","iopub.execute_input":"2022-05-29T12:09:17.290687Z","iopub.status.idle":"2022-05-29T12:19:55.740342Z","shell.execute_reply.started":"2022-05-29T12:09:17.290647Z","shell.execute_reply":"2022-05-29T12:19:55.739557Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"evaluation_result = trainer.evaluate(test_dataset)\nprint(evaluation_result)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:19:55.745923Z","iopub.execute_input":"2022-05-29T12:19:55.748542Z","iopub.status.idle":"2022-05-29T12:20:09.344549Z","shell.execute_reply.started":"2022-05-29T12:19:55.748503Z","shell.execute_reply":"2022-05-29T12:20:09.343851Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n\nresults = trainer.predict(test_dataset)\n\nscores = [softmax(prediction) for prediction in results.predictions]\npredicted_labels = [np.argmax(x) for x in scores]\npredicted_labels[0:21]\n","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:20:09.345602Z","iopub.execute_input":"2022-05-29T12:20:09.346085Z","iopub.status.idle":"2022-05-29T12:20:24.938636Z","shell.execute_reply.started":"2022-05-29T12:20:09.346049Z","shell.execute_reply":"2022-05-29T12:20:24.937875Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"csv_output = np.insert(scores, 0, predicted_labels, axis=1)\ndf = pd.DataFrame(csv_output)\ndf[0] = df[0].astype(\"int\")\nheader = [\"predicted_label\"]\nfor label in label2id.keys():\n    header.append(label)\nheader","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:20:24.939924Z","iopub.execute_input":"2022-05-29T12:20:24.944479Z","iopub.status.idle":"2022-05-29T12:20:24.971830Z","shell.execute_reply.started":"2022-05-29T12:20:24.944441Z","shell.execute_reply":"2022-05-29T12:20:24.971088Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"df.to_csv(r\"./result.csv\", index=False, header=header)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:20:24.975640Z","iopub.execute_input":"2022-05-29T12:20:24.976236Z","iopub.status.idle":"2022-05-29T12:20:25.058952Z","shell.execute_reply.started":"2022-05-29T12:20:24.976196Z","shell.execute_reply":"2022-05-29T12:20:25.058097Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"dataset=pd.read_csv(r\"./result.csv\")\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:20:25.064408Z","iopub.execute_input":"2022-05-29T12:20:25.069038Z","iopub.status.idle":"2022-05-29T12:20:25.096386Z","shell.execute_reply.started":"2022-05-29T12:20:25.068997Z","shell.execute_reply":"2022-05-29T12:20:25.095714Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def get_predictions():\n    predicted_labels = []\n    scores = []\n    data =  pd.read_csv(r\"./result.csv\", header=0).values.tolist()\n    for row in data:\n        predicted_labels.append(int(row[0]))\n        scores.append([float(el) for el in row[1:]])\n    return predicted_labels,scores\npredicted_labels,scores = get_predictions()\nprint(predicted_labels[0:20])","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:20:25.099092Z","iopub.execute_input":"2022-05-29T12:20:25.101928Z","iopub.status.idle":"2022-05-29T12:20:25.151183Z","shell.execute_reply.started":"2022-05-29T12:20:25.101890Z","shell.execute_reply":"2022-05-29T12:20:25.150457Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def main(task=\"NLI_M\", dataset_type=\"sentihood\"):\n    predicted_labels, scores = get_predictions()\n    test_original_sentences = list(auxdata_test.iloc[:,0])\n    test_auxiliary_sentences = list(auxdata_test.iloc[:,1])\n    test_labels = list(auxdata_test.iloc[:,2])\n    if dataset_type == \"sentihood\":\n        sentihood_aspect_strict_acc = compute_sentihood_aspect_strict_accuracy(test_labels, predicted_labels)\n        print(f\"{task} Sentihood aspect strict accuracy: {sentihood_aspect_strict_acc}\")\n        sentihood_aspect_macro_F1 = compute_sentihood_aspect_macro_F1(test_labels, predicted_labels)\n        print(f\"{task} Sentihood aspect macro F1: {sentihood_aspect_macro_F1}\")\n        sentihood_aspect_macro_AUC = compute_sentihood_aspect_macro_AUC(test_labels, scores)\n        print(f\"{task} Sentihood aspect macro AUC: {sentihood_aspect_macro_AUC}\")\n\n        sentihood_sentiment_macro_AUC, sentihood_sentiment_accuracy = compute_sentihood_sentiment_classification_metrics(\n            test_labels, scores)\n        print(f\"{task} Sentihood sentiment accuracy: {sentihood_sentiment_accuracy}\")\n        print(f\"{task} Sentihood sentiment macro AUC: {sentihood_sentiment_macro_AUC}\")\n\nmain(\"NLI_M\", \"sentihood\")","metadata":{"execution":{"iopub.status.busy":"2022-05-29T12:20:25.154946Z","iopub.execute_input":"2022-05-29T12:20:25.155563Z","iopub.status.idle":"2022-05-29T12:20:25.298260Z","shell.execute_reply.started":"2022-05-29T12:20:25.155524Z","shell.execute_reply":"2022-05-29T12:20:25.294877Z"},"trusted":true},"execution_count":35,"outputs":[]}]}
